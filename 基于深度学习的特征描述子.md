



## 数据如何准备

**数据集生成**：三维重建后，能够知道每个3D点所对应的2D点，同一个3D点所对应的2D点都认为是匹配点（当然不可能保证百分百是正确匹配）。根据每个2D点的特征点所对应的角度和尺度，根据尺度，能够知道每个特征点需要多大的图像区域来计算特征描述子，但是神经网络需要一致大小的输入，所以将该区域resize到32x32。并且根据特征点的角度对patch进行旋转。



一个3D点对应的2D点可能多，也就说这些2D点的匹配关系是稳定的，错误匹配的概率低。所以刚开始认为从匹配点多的2D点中取patch。

后来经过思考后，在一个3D点对应的2D点少的情况下；有两种情况，匹配难度大，或者是错误匹配。匹配难度大的样本对于训练是有必要的，所以需要从中选取patch。但是为了避免选到错误的匹配，要对选取的对应patch进行一定的校验，（人眼过一遍进行校验）。



并且大部分的3D点对应的2D点的数目都是比较少的。



最终，选取出50万个左右个3D点，150万左右个pathch。





## 网络结构

L2-Net中说BN不能够使用仿射变换，　所以HardNet中也是不用的，我改了一下用仿射变换，发现好像没什么影响。







## 如何构造loss/如何挑选batch中的数据构造样本对

使用了triplet loss + online batch mining



对于每个batch，其batch中的每个元素是一个匹配对，对于一个batch算出一个距离矩阵，其对角线上的元素是匹配对的距离，非对角线的元素是非匹配对的距离。我们在这个矩阵上构造loss。



一个想法就是希望对角线元素的距离尽可能小，非对角线元素的距离尽可能大。



但是，其实，我们只需要保证匹配对的距离小于非匹配对的距离就行了。

所以，我们只需要优化那些距离比较小的非匹配对就可以了。



更进一步，只去优化那些距离最小的非匹配样本就行了。





## 训练设置

batch size 为 1024或者512。大的batch size对triplet loss是有利的。

优化器使用SGD，学习率随着迭代次数的增加而线性减小。



## 如何评价性能

FPR95

mAP

运行时间









[Triplet Network, Triplet Loss及其tensorflow实现](https://zhuanlan.zhihu.com/p/35560666)

[Facenet即triplet network模型训练，loss不收敛的问题？](https://www.zhihu.com/question/38937343)

[Understanding Ranking Loss, Contrastive Loss, Margin Loss, Triplet Loss, Hinge Loss and all those confusing names](https://gombru.github.io/2019/04/03/ranking_loss/)

[Understanding Categorical Cross-Entropy Loss, Binary Cross-Entropy Loss, Softmax Loss, Logistic Loss, Focal Loss and all those confusing names](https://gombru.github.io/2018/05/23/cross_entropy_loss/)



[Siamese Network & Triplet Loss](https://towardsdatascience.com/siamese-network-triplet-loss-b4ca82c1aec8)