[TOC]



## Motivation

尺度不变性

旋转不变性

光照不变性

仿射不变性



## 数据集的准备

因为我之前实现了三维重建的程序，因此我们数据集的生成利用了我之前实现的三维重建的程序。三维重建之后，能够得到3D点云，并且知道哪张图像的2D特征点看到了该3D点。对于看到了相同3D点的2D特征点，可以认为其是匹配点，也即物理世界中一个3D点在不同图像中的投影点。所以，我先生成一个list文件，将匹配点的索引存在一行中。然后，接着根据这些索引，找到相应的图像及其特征点，然后根据特征点的尺度和角度，在特征点的周围提取patch，然后根据角度进行旋转，最后resize到32x32大小。



那么为什么选择32x32大小呢？开源的数据集是选择缩放到32x32大小。我统计了一下我数据集中尺度大小的分布，发现其尺度主要集中在20多到30多之间，所以我选择resize到32x32大小。





最终，选取出50万个左右个3D点，250万左右个patch。patch大小为32 x 32，每张图像32 x 32 = 1024个patch，每张图像的大小为1024 x 1024。最后总共有2500多张图像，每张图像1M，所以共2.5个G左右的数据集。



灰度图是2.5个G，那么彩色图呢。彩色图，每张图像2.5M左右，所以彩色数据集是6个多G大小， 也就是灰度图数据集的2点多倍。





## 数据增强

尺度不变性－－随机高斯模糊

旋转不变性－－随机旋转

光照不变性－－Color Jittering，对颜色的数据增强：图像亮度、饱和度、对比度变化

仿射不变性－－仿射变换



## 网络结构的设计

![1568620900008](assets/基于深度学习的特征描述子.asset/1568620900008.png)

网络结构和L2Net一致，都是7层神经网络，并且最后一层没有激活函数。有试验过将修改网络结构，其中一个修改就是将网络结构改成3到4层，然后发现精度并没有下降。说明对于学习特征描述子来说，这么个简单的网络已经足够了。所以，我就继续沿用了L2Net的网络结构。





## Loss的设计

输入的结构为：

```python
一个batch的大小为n,同一行的两个图像对的匹配的，不同行的任意两个图像对都是不匹配的
[
    (a1, p1),
    (a2, p2),
    (a3, p3),
    ...
    (an, bn),
]

计算得到距离矩阵
[
    [ dist(a1, p1), dist(a1, p2), dist(a1, p3), ..., dist(a1, pn) ],
    [ dist(a2, p1), dist(a2, p2), dist(a2, p3), ..., dist(a2, pn) ],
    [ dist(a3, p1), dist(a3, p2), dist(a3, p3), ..., dist(a3, pn) ],
    ...
    [ dist(an, p1), dist(an, p2), dist(an, p3), ..., dist(an, pn) ],
]


```



L2Net使用的softmax损失，也就说使得对角线上的损失尽量小（匹配对），非对角线上的元素尽量大（非匹配对）。

但是，这个问题其实是一个正负样本不均衡的问题。对于batch大小为n的输入，其计算得到出来的距离矩阵，只有n个正样本；而剩余的n^2 - n个都负样本。L2Net并没有处理这个问题，只是单纯只用softmax损失来最小化正样本，最大化负样本。因此，batch越大，正负样本越不均衡，因此这样的学习并不是高效的。



> 那么为什么把batch size调小呢？
>
> 这并不能根本上解决问题，因为我们训练的是最小化匹配对的距离，最大化非匹配对的距离。
>
> 数据集中匹配对和非匹配对的数量都是固定的，而batch size的大小只是决定这个batch能够访问到多大的匹配对和非匹配对。



所以，我对其进行了改进，在loss中加入了hard mining。使其能够更加高效地进行学习。具体的loss为，最小化对角线，然后在每一行中挑选出loss最小的非匹配对，然后最大化这个loss。



> 需要进一步了解正负样本不均衡所造成的影响。





## 训练设置

batch size 为 1024或者512。大的batch size对triplet loss是有利的。

优化器使用SGD，学习率随着迭代次数的增加而线性减小。

测试的时候，每秒跑4次迭代，每次迭代1024个batch_size， 也就是每秒跑4096个patch，由于每张图像196个patch，所以每秒20多张图像左右。



hardnet是16w个3D点，生成了5百万个triplet，训练一个epoch20多分钟。



实际场景测试时，因为我们的图像比较大，每个图像5000到1万个特征点，那么每张图像需要1到2s来计算描述子。



学习率的调整方案：

优化器使用的是SGD， 首先确定初始化的学习率， 然后我发现，学习率可以设为10这么大， 并且收敛得很快。 之所以学习率可以设置得比较大， 我分析是因为我用了BN，更主要的是我的网络比较浅，只有几层。



然后接下来的调整使用了两种方案，  一种就是不断迭代，等loss不下降了然后将学习率除以10，另外一种就是poly学习率调整。  后者比前者要好一些，





## 获得的提升



### 收敛速度的提升

如果是使用L2Net，那么需要30多个epoch才能够收敛。　如果改成我刚才说的那个loss，那么只需要迭代2-3个epoch就能够收敛了，　也就是达到了FPR95 = 3%的效果。

并且，每个epoch需要40分钟左右。所以将总的迭代时间从20个小时缩小到2个小时左右，这是个很大的提升。



### FPR95的降低

相对于SIFI算法，将匹配点的FPR95从26%降低到3%；相对于L2Net将匹配点的FPR95从6%降低到3%；并提高重建完整度10%。



加了一些数据增强，将FPR95%从3%降低到了2%



**FPR95的定义**

recall=95%时，1 - presion的大小是多少。

**如何理解FPR95**

**如何计算FPR95**



