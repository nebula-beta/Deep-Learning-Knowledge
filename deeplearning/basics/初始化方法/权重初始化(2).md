[TOC]

#  权重初始化(2)



## Xavier初始化

Paper : [Understanding the difficulty of training deep feedforward neural networks](http://proceedings.mlr.press/v9/glorot10a.html)

Xvaier在$\frac{1}{\sqrt{n_{in}}}$初始化方法的基础上考虑了反向传播的梯度输入输出的方差一致性。

也即Xavier初始化方差考虑了正向传播的输入输出方差一致性和反向传播的梯度输入输出方差一致性。

由于正向的推导和$\frac{1}{\sqrt{n_{in}}}$初始化方法一样，并且得出的结论也一样，所以下面只推导反向传播。
$$
z_1 = W_1x + b_1
$$

$$
z_2 = W_2z_1 + b_2
$$

$$
...
$$



那么反向传播有：
$$
\frac{\partial Cost}{\partial z_1} = \frac{\partial Cost}{\partial z_2} \cdot \frac{\partial z_2}{\partial z_1}
$$

$$
\frac{\partial Cost}{\partial z_1} = W_2^T \cdot \frac{\partial Cost}{\partial z_2}  
$$

那么对第一层的第$j$个神经元的导数应该为：
$$
\frac{\partial Cost}{\partial z_1^j} = \sum_i^{n_{out}} (W_2^{ij})^T \cdot \frac{\partial Cost}{\partial z_2^{i}}  
$$
那么其方差为：
$$
Var(\frac{\partial Cost}{\partial z_1^j}) = Var(\sum_i^{n_{out}} (W_2^{ij})^T \cdot \frac{\partial Cost}{\partial z_2^{i}} ) 
$$
为了简便起见，省略上标，那么有：
$$
Var(\frac{\partial Cost}{\partial z_1}) = Var(\sum_i^{n_{out}} W_2^T \cdot \frac{\partial Cost}{\partial z_2}  )
$$

$$
Var(\frac{\partial Cost}{\partial z_1}) = n_{out} \cdot Var(W_2^T \cdot \frac{\partial Cost}{\partial z_2}  )
$$

根据$Var(x)=E(x^2)-E^2(x)$，得到：
$$
Var(\frac{\partial Cost}{\partial z_1}) = n_{out} \cdot [E(W_2^T \cdot \frac{\partial Cost}{\partial z_2})^2 - E^2(W_2^T \cdot \frac{\partial Cost}{\partial z_2})]
$$
根据独立性，有$E(xy)=E(x)E(y)$，得到：
$$
Var(\frac{\partial Cost}{\partial z_1}) = n_{out} \cdot [E(W_2^T)^2 \cdot E(\frac{\partial Cost}{\partial z_2})^2 - E^2(W_2^T) \cdot E^2(\frac{\partial Cost}{\partial z_2})]
$$
根据权重的期望为0，得到：
$$
Var(\frac{\partial Cost}{\partial z_1}) = n_{out} \cdot [E(W_2^T)^2 \cdot E(\frac{\partial Cost}{\partial z_2})^2]
$$
再次根据$Var(x)=E(x^2)-E^2(x)$，得到：
$$
Var(\frac{\partial Cost}{\partial z_1}) = n_{out} \cdot [(Var(W_2^T) + E^2(W_2^T)) \cdot (Var(\frac{\partial Cost}{\partial z_2}) + E^2(\frac{\partial Cost}{\partial z_2}))]
$$
根据$\frac{\partial Cost}{z_2}=\frac{\partial Cost}{\partial z_3} \cdot \frac{\partial z_3}{\partial z_2}=W_3^T \frac{\partial Cost}{\partial z_3}$，并且由于权重的期望为0，所以$E(\frac{\partial Cost}{\partial z_2})= E(W_3^T \cdot \frac{\partial Cost}{\partial z_3})=0$，带入上式，得到:
$$
Var(\frac{\partial Cost}{\partial z_1}) = n_{out} \cdot Var(W_2^T)  \cdot Var(\frac{\partial Cost}{\partial z_2})
$$
为了让前后两层梯度的方差一样，那么需要：
$$
 n_{out} \cdot Var(W_2^T) = 1
$$
从而得到约束条件有：
$$
Var(W_2^T) = \frac{1}{ n_{out}}
$$


那么，从前向和后向推导就能够得到两个约束条件：
$$
Var(W_2) = \frac{1}{n_{in} }
$$

$$
Var(W_2^T) = \frac{1}{ n_{out}}
$$

一般网络的$n_{in}$和$n_{out}$是不一样的，所以这两个约束条件没有办法同时满足，所以论文的作者使用了调和平均数（什么是调和平均数，为什么要使用它？）
从而得到新的约束条件：
$$
Var(W_2) = \frac{2}{n_{in} + n_{out} }
$$

>  所以，Xavier初始化方法使用高斯分布$N(0,\frac{2}{n_{in} + n_{out} })$，或者是均匀分布$U(-\frac{\sqrt6}{\sqrt{n_{in} + n_{out}}}, \frac{\sqrt6}{\sqrt{n_{in} + n_{out}}})$

Xavier所适合的激活函数有限：

1. 关于0对称
2. 线性，也就是没有考虑激活涵，或者初始状态在激活函数的线性部分

ReLU并不满足这些条件，实验也验证了Xavier初始化确实不适用ReLU激活函数。





## 参考

[神经网络权重初始化问题 - marsggbo的博客 - CSDN博客](https://blog.csdn.net/marsggbo/article/details/77771497)

[自编码器参数初始化方法-Xavier initialization - 简书](https://www.jianshu.com/p/4e53d3c604f6)

[Xavier 初始化方法 \| davidlau's blog](http://davidlau.me/2019/03/21/xavier/)

[Xavier初始化 - 咖木的文章 - 知乎](https://zhuanlan.zhihu.com/p/32339947)

