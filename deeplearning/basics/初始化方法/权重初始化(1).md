[TOC]



# 权重初始化方法(1)

1. 初始化为0
2. $\frac{1}{\sqrt{n_{in}}}$初始化方法
3. Xavier初始化
4. Kaiming初始化



TODO : 

1. BN和初始化方法是否矛盾
2. BN该如何初始化(一般是$\gamma=1, \beta=0$)
3. 卷积核该如何初始化（卷积能够转换成矩阵乘法，所以可以使用相同的方法进行初始化）
4. Xavier方法如何使用与不同的激活函数
5. 为什么要保正向和方向的方差(因为无论正向还是反向都存在权重的连乘，为了避免连乘导致过大或者多小，所以需要保持方差一致性)

## 初始化为常数（例如0）

$$
z_1 = W_1 x + b1
$$

$$
a_1 = f(z_1)
$$

$$
z_2 = W_2 a_1 + b_2
$$

$$
a_2 = f(z_2)
$$

$$
L = \frac{1}{2}||y-a_2||^2
$$

##### 分析对$W$的导数

$$
\frac{\partial L}{\partial W_2}= \frac{\partial L}{\partial a_2} \cdot  \frac{\partial a_2}{\partial z_2} \cdot  \frac{\partial z_2}{\partial W_2}
$$

$$
=-(y-a_2)f'(z_2)a_1^T
$$

如果使用常数初始化，那么导致同层神经元的输出都是相同的，例如$a_2$的每个维度相同、$z_2$的每个维度相同和，$a_1$的每个维度相同，从而导致梯度都是相同的，从而导致 __每层__ 的权重的更新也是相同的。

更特殊的，如果权重初始化为0，且激活函数为ReLU，那么权重不会被更新，因为梯度为0.

##### 分析对$b$的导数

同理，对偏置$b_2$的梯度进行分析：
$$
\frac{\partial L}{\partial b_2}= \frac{\partial L}{\partial a_2} \cdot  \frac{\partial a_2}{\partial z_2} \cdot  \frac{\partial z_2}{\partial b_2}
$$

$$
=-(y-a_2)f'(z_2)
$$

每层bias的更新都是相同的，不过和权重$W$相比，就算初始权重为0，且激活函数为ReLU，bias也会被更新。



## $\frac{1}{\sqrt{n_{in}}}$初始化方法

一些假设

> 1. 激活函数是关于0对称的
> 2. 初始化状态落在激活函数的线性部分，，或者不存在激活函数
> 3. 输入元素x的方差相同，每层权重元素的方差也相同
> 4. W和x的均值都为0，且相互独立
> 5. bias均值和方差都为0，也即初始化为0.

因为前两个假设的存在，所以我们可以将激活函数认为是恒等映射，也即不存在激活函数。

有下面的两层神经网络，由于$x$是输入，是可以进行预处理的，比如说预处理成0均值1方差的。那么权重初始化时，需要将权重$W$初始化成什么值，才能使得每层的输出$z_1$和$z_2$的方差和输入$x$的方差是一样的呢。

也即 __输入输出方差一致性__。
$$
z_1 = W_1x + b_1 
$$

$$
z_2 = W_2z_1 + b_2
$$

$$
...
$$

对于第二层第$i$个神经元的输出$z_2^i$，有
$$
z_2^i = \sum_j^{n_{in}}W_2^{ij}z_1^j + b_2^i
$$
那么其方差为：
$$
Var(z_2^i) = Var(\sum_j^{n_{in}}W_2^{ij}z_1^j + b_2^i)
$$
为了简便，省略上标，那么有：
$$
Var(z_2) = Var(\sum_j^{n_{in}}W_2 z_1 + b_2)
$$
$n_{in}$表示输入神经元的公式，每个输出神经元都由$n_{in}$个输入神经元经过加权得到
$$
Var(z_2) = n_{in}Var(W_2 z_1 + b_2)
$$

$$
Var(z_2) = n_{in}Var(W_2 z_1)
$$

使用公式 __$Var(x) = E(x)^2 -E^2(x)$__，得到：
$$
Var(z_2) = n_{in}[E(W_2 z_1)^2 - E^2(W_2z_1)]
$$
运用$W$和$x$的独立性，得到：
$$
Var(z_2) = n_{in}[E(W_2)^2 E(z_1)^2 - E^2(W_2)E^2(z_1)]
$$
运用权重的期望为0的性质$E(W)=0$，得到：
$$
Var(z_2) = n_{in}[E(W_2)^2 E(z_1)^2]
$$
再次使用公式 __$Var(x) = E(x)^2 -E^2(x)$__，得到：
$$
Var(z_2) = n_{in}[(Var(W_2) +E^2(W_2)) \cdot (Var(z_1)+E^2(z_1))]
$$
使用权重和输入的均值为0的假设，得到：
$$
Var(z_2) = n_{in}\cdot Var(W_2) \cdot Var(z_1)
$$
为了让输出和输入的方差一样，那么就要有：
$$
Var(z_2) = Var(z_1)
$$
从而推出权重的方差需要满足下面的约束：
$$
n_{in} \cdot Var(W_2) = 1
$$
所以有：
$$
Var(W_2) = \frac{1}{n_{in} }
$$

> 所以可以用高斯分布$N(0, \frac{1}{n_{in} })$或均匀分布$U(-\frac{\sqrt 3}{\sqrt{n_{in}}}, \frac{\sqrt 3}{\sqrt{n_{in}}})$来对权重进行初始化，从而保证 __输入输出方差一致性__。



那么为什么要__保证__输入输出方差一致性呢？对于L层的神经网络，有：
$$
Var(z_l)=Var(x)\prod_{l=1}^{L}n_l^{in}Var(W_l)
$$

> 因为连乘的存在，所以若$n_l^{in}Var(W_l) \ne 1$，其太大或者太小都会知道深层神经网络的输出出现问题，这也就是为什么要保证输入和输出方差一致性。

###  实验

下面使用两种方法进行初始化，在全连接网络上进行测试，分别测试了激活函数为`Sigmoid`、`Tanh`和`ReLU`下的效果。

全连接神经网络的参数为`layers_dims = [1000,800,500,300,200,100,90,80,40,20,10]`，除去输入层，神经网络为`10`层，将`10000`个数据输入到该神经网络中，并记录每层输出分布的直方图，下面将这`10`层输出值的直方图画了出来。

使用了两种权重初始化方法

1. $N(\mu =0, \sigma^2=0.01^2)$
2. $N(\mu =0, \sigma^2=\frac{1}{n_{in}})$

##### 激活函数为sigmoid

下面是使用__第一种__权重初始化方法所得到的每层输出值直方图，可见，在激活函数为`Sigmoid`的情况下，随着网络的加深，这种初始化方法并不能得到很好的结果。

![image-20190706162002495](../../../assets/权重初始化(1).assert/image-20190706162002495.png)

下面是使用__第二种__权重初始化所得到的每层输出值直方图，在激活函数为`Sigmoid`的情况下，随着网络的加深，能够得到还行的结果，至少比__第一种__方法要强，如果其实也能看出来，如果网络更深，那么结果其实会变坏。



![image-20190706162034602](../../../assets/权重初始化(1).assert/image-20190706162034602.png)



##### 激活函数为tanh

下面是使用__第一种__权重初始化方法所得到的每层输出值直方图，可见，在激活函数为`Tanh`的情况下，随着网络的加深，这种初始化方法并不能得到很好的结果。

![image-20190706162227407](../../../assets/权重初始化(1).assert/image-20190706162227407.png)

下面是使用__第二种__权重初始化所得到的每层输出值直方图，在激活函数为`Tanh`的情况下，随着网络的加深，能够得到很好的结果。

![image-20190706162259579](../../../assets/权重初始化(1).assert/image-20190706162259579.png)



##### 激活函数为ReLU

下面是使用__第一种__权重初始化方法所得到的每层输出值直方图，可见，在激活函数为`ReLU`的情况下，随着网络的加深，这种初始化方法并不能得到很好的结果。

![image-20190706162402020](../../../assets/权重初始化(1).assert/image-20190706162402020.png)

下面是使用__第二种__权重初始化所得到的每层输出值直方图，该方法虽然在激活函数为`tanh`的深层网络上取得不错的效果，但是却对激活函数为`ReLU`的深层网络表现并不好。

![image-20190706162439309](../../../assets/权重初始化(1).assert/image-20190706162439309.png)



### 结论

无论激活函数是`Sigmoid`、`Tanh`还是`ReLU`，第一种固定方差的随机初始化方法都不能取得很好的效果。而第二种方法也只在激活函数为`Tanh`的深层网络上表现良好。这是因为在推导该权重初始化方法的过程中，存在一些假设：

1. 激活函数是关于0对称的
2. 初始化状态落在激活函数的线性部分，或者不存在激活函数
3. 输入元素x的方差相同，每层权重元素的方差也相同
4. W和x的均值都为0，且相互独立
5. bias均值和方差都为0，也即初始化为0.



其中假设`1`和假设`2`对于`Sigmoid`和`ReLU`并不是成立的，所以推导得到的方差一致性的条件并不适用于这两个激活函数，所以该权重初始化方法并不适用于这两种激活函数；而对于激活函数`Tanh`则满足这两条假设的，所以这种初始化方法在激活函数为`Tanh`的深层网络上取得了很好的结果。

## 参考

[为什么神经网络参数不能全部初始化为全0？ - 知乎](https://zhuanlan.zhihu.com/p/27190255)

[TensorFlow从0到1 - 15 - 重新思考神经网络初始化 - 简书](https://www.jianshu.com/p/465f31d54eeb?utm_campaign=maleskine&utm_content=note&utm_medium=seo_notes&utm_source=recommendation)

[深度学习中神经网络的几种权重初始化方法 - 天泽28的专栏 - CSDN博客](https://blog.csdn.net/u012328159/article/details/80025785)