[TOC]

# Random Forest



随机森林使用Bagging策略来生成多棵决策树，从而组成随机森林。

对训练集的进行采样，对特征选择子集，然后生成一棵决策树。 



1. 从样本集上有重采样（有重复的）选出n个样本；
2. 在所有属性上，对着n个样本建立分类器（__ID3__, __C4.5__, __CART__,  SVM, Logistic回归等）；
3. 重复以上两个步骤m次，即获得了m个分类器；
4. 将数据放到这m个分类器上，最后根据这m个分类器的投票结果，决定数据属于哪一类。



__为什么是有放回的采样__：

因为我们想要每次喂给分类器的训练集是不一样的，因为训练集不一样才能够训练出有差异的分类器。



随机森林的基分类器，差异越大越能有好的结果。如果很多特征之间相关性较强，用较少的特征，能训练出差异较大的基分类器。



不推荐使用Bagging的方式组合SVM，因为随机采样得到的样本包含支撑向量的话，那么得到的分类器其实是一样的。



> 在建立每一棵决策树的过程中，有两点需要注意 – 采样与完全分裂。首先是两个随机采样的过程，random forest对输入的数据要进行行、列的采样。对于行采样，采用有放回的方式，也就是在采样得到的样本集合中，可能有重复的样本。假设输入样本为N个，那么采样的样本也为N个。这样使得在训练的时候，每一棵树的输入样本都不是全部的样本，使得相对不容易出现over-fitting。然后进行列采样，从M个feature中，选择m个(m << M)。之后就是对采样之后的数据使用完全分裂的方式建立出决策树，这样决策树的某一个叶子节点要么是无法继续分裂的，要么里面的所有样本的都是指向的同一个分类。一般很多的决策树算法都一个重要的步骤 – 剪枝，但是这里不这样干，由于之前的两个随机采样的过程保证了随机性，所以就算不剪枝，也不会出现over-fitting。
> 作者：QQ595454159链接：https://www.imooc.com/article/42949来源：慕课网





## 参考

[为什么说bagging是减少variance，而boosting是减少bias? - 过拟合的回答 - 知乎](https://www.zhihu.com/question/26760839/answer/40337791)

