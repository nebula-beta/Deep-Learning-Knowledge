[TOC]

# 权重初始化方法(3)

## Kaiming初始化 

Paper : [Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification](http://xxx.itp.ac.cn/abs/1502.01852)

Kaiming初始化是针对ReLU网络的初始化方法。

之前的推导并没有在推导中加入激活函数，下面会在推导中加入激活函数。



> 需要注意的是，之前推导Xavier初始化方法时，不管正向推导还是反向推导都是假设不存在激活函数，也就说所求结论都是为了保证正向传播时：状态值的方差保持不变；反向传播时：状态值的方差保持不变。
>
> 而在Kaiming初始化方法的推导中，存在激活函数，所求条件的适用性发生了变化。正向传播时：状态值的方差保证不变；反向传播时：激活值的方差保持不变。
>
> Note：状态值激活得到激活值。





#### 正向推导

$$
z_1 = W_1x + b_1
$$

$$
a_1 = f(z_1)
$$

$$
z_2 = W_2 a_1 + b_2
$$

$$
a_2 = f(z_2)
$$

$$
...
$$



有前面的推导基础在，那么直接省略一大堆推导，直接得到下面的公式：
$$
Var(z_2) = n_{in} \cdot Var(W_2 a_1)
$$
和之前唯一的区别就是这里考虑了激活函数，所以公式里的值是$a_1$,而不是$z_1$。
$$
Var(z_2) = n_{in} \cdot Var(W_2 a_1)
$$

$$
Var(z_2) = n_{in} \cdot [E(W_2 a_1)^2 - E^2(W_2a_1)]
$$

$$
Var(z_2) = n_{in} \cdot [E(W_2)^2 E(a_1)^2 - E^2(W_2)E^2(a_1))]
$$

$$
Var(z_2) = n_{in} \cdot E(W_2)^2 E(a_1)^2 
$$

$$
Var(z_2) = n_{in} \cdot [Var(W_2) + E^2(W_2)] \cdot E(a_1)^2 
$$

$$
Var(z_2) = n_{in} \cdot Var(W_2) \cdot E(a_1)^2 
$$

因为$a_1$是$z_1$通过激活函数（ReLU)得到的，所以$E(a_1)^2$需要算一算，其计算如下：
$$
E(a_1)^2 = E(f(z_1))^2
$$

$$
= \int_{-\infty}^{+\infty} p(z_1)(f(z_1))^2 d z_1
$$

$$
= \int_{-\infty}^{0} p(z_1)(f(z_1))^2 d z_1 
+
\int_{0}^{+\infty} p(z_1)(f(z_1))^2 d z_1
$$

$$
= 0 
+
\int_{0}^{+\infty} p(z_1)(z_1)^2 d z_1
$$

$$
=
\frac{1}{2}\int_{-\infty}^{+\infty} p(z_1)(z_1)^2 d z_1
$$

$$
=\frac{1}{2}E(z_1)^2
$$

$$
=\frac{1}{2}(Var(z_1) + E^2(z_1))
$$

$$
=\frac{1}{2}Var(z_1) 
$$

也即
$$
E(a_1)^2 =\frac{1}{2}Var(z_1) 
$$
将其带入到
$$
Var(z_2) = n_{in} \cdot Var(W_2) \cdot E(a_1)^2 
$$
中得到：
$$
Var(z_2) = \frac{n_{in}}{2} \cdot Var(W_2) \cdot Var(z_1)  
$$
为了让正向传播时状态值的方差不变，那么权重的方差需要满足:
$$
Var(W_2) = \frac{2}{n_{in}}
$$

#### 反向推导

$$
z_1 = W_1x + b_1
$$

$$
a_1 = f(z_1)
$$

$$
z_2 = W_2a_1 + b_2
$$

$$
a_2 = f(z_2)
$$

那么反向传播有：
$$
\frac{\partial Cost}{\partial a_1} = \frac{\partial Cost}{\partial a_2} \cdot \frac{\partial a_2}{\partial z_2} \frac{\partial z_2}{\partial a_1}
$$

$$
\frac{\partial Cost}{\partial a_1} = W_2^T \cdot \frac{\partial Cost}{\partial a_2} \cdot f'(z_2)
$$

那么对第一层的第$j$个神经元的__激活值__导数应该为：
$$
\frac{\partial Cost}{\partial a_1^j} = (\sum_i^{n_{out}} (W_2^{ij})^T \cdot \frac{\partial Cost}{\partial a_2^{i}} ) \cdot f'(z_2^j)
$$
那么其方差为：
$$
Var(\frac{\partial Cost}{\partial a_1^j}) = Var(\sum_i^{n_{out}} (W_2^{ij})^T \cdot \frac{\partial Cost}{\partial a_2^{i}} \cdot f'(z_2^j))
$$
为了简便起见，省略下标，从而得到：
$$
Var(\frac{\partial Cost}{\partial a_1}) = Var(\sum_i^{n_{out}} W_2^T \cdot \frac{\partial Cost}{\partial a_2}  \cdot f'(z_2))
$$

$$
Var(\frac{\partial Cost}{\partial a_1}) = n_{out} \cdot Var(W_2^T \cdot \frac{\partial Cost}{\partial a_2} \cdot f'(z_2) )
$$

接着使用__方差=平方的期望 - 期望的平方__ __$Var(x)=E(x^2)-E^2(x)$__，从而得到：
$$
Var(\frac{\partial Cost}{\partial a_1}) = n_{out} \cdot [E(W_2^T \cdot \frac{\partial Cost}{\partial a_2} \cdot f'(z_2))^2 - E^2(W_2^T \cdot \frac{\partial Cost}{\partial a_2} \cdot f'(z_2))]
$$

$$
Var(\frac{\partial Cost}{\partial a_1}) = n_{out} \cdot [E(W_2^T)^2 \cdot E(\frac{\partial Cost}{\partial a_2})^2 \cdot E(f'(z_2))^2- E^2(W_2^T) \cdot E^2(\frac{\partial Cost}{\partial a_2}) \cdot E^2(f'(z_2))]
$$

由于权重的均值为0，$E(W_2^T)=0$，从而得到：
$$
Var(\frac{\partial Cost}{\partial a_1}) = n_{out} \cdot [E(W_2^T)^2 \cdot E(\frac{\partial Cost}{\partial a_2})^2 \cdot E(f'(z_2))^2]
$$
由于激活函数是ReLU，所以$f'(z_2)$的取值为0或者1，所以$(f'(z_2))^2$的取值也是0或者1，所以有：
$$
E(f'(z_2))^2 = \frac{1}{2}
$$
因此：
$$
Var(\frac{\partial Cost}{\partial a_1}) = \frac{n_{out}}{2}  \cdot [E(W_2^T)^2 \cdot E(\frac{\partial Cost}{\partial a_2})^2]
$$
再次使用__方差=平方的期望 - 期望的平方__ __$Var(x)=E(x^2)-E^2(x)$__，从而得到：
$$
Var(\frac{\partial Cost}{\partial a_1}) = \frac{n_{out}}{2} \cdot [(Var(W_2^T) + E^2(W_2^T)) \cdot (Var(\frac{\partial Cost}{\partial a_2}) + E^2(\frac{\partial Cost}{\partial a_2}))]
$$
因为$\frac{\partial Cost}{\partial a_2}=\frac{\partial Cost}{\partial z_3} \cdot \frac{\partial z_3}{\partial a_2}=W_3^T \cdot \frac{\partial Cost}{\partial z_3}$，并且由于权重的期望为0，所以$E(\frac{\partial Cost}{\partial a_2})= E(W_3^T \cdot \frac{\partial Cost}{\partial z_3})=0$，带入上式，得到:
$$
Var(\frac{\partial Cost}{\partial a_1}) = \frac{n_{out}}{2} \cdot Var(W_2^T)  \cdot Var(\frac{\partial Cost}{\partial a_2})
$$
得到约束：
$$
\frac{n_{out}}{2} \cdot Var(W_2^T)  = 1
$$


也即：
$$
Var(W_2^T)  = \frac{2}{n_{out}}
$$


综上，就得到两个约束
$$
Var(W_2) = \frac{2}{n_{in}}
$$

$$
 Var(W_2^T)  = \frac{2}{n_{out}}
$$



但是，Kaiming初始化并没有想Xavier初始化像Xavier初始化那样，取两者的调和平均数。取决于你想要保证前向传播输入和输出的范数还是反向传播输入和输出的范数，从而使用不同的约束条件。

> 使用高斯分布$N(0, \frac{2}{n_{in}})$或者均匀分布$U(-\frac{\sqrt{6}}{\sqrt{n_{in}}},\frac{\sqrt{6}}{\sqrt{n_{in}}})$来保证前向传播方差的一致性。

> 或者使用高斯分布$N(0, \frac{2}{n_{out}})$或者均匀分布$U(-\frac{\sqrt{6}}{\sqrt{n_{out}}},\frac{\sqrt{6}}{\sqrt{n_{out}}})$来保证反向传播方差的一致性。



#### 为何Kaiming初始化不使用像Xavier的调和平均数呢？

$$
z_1 = W_1 x + b_1
$$

$$
a_1 = f(z_1)
$$

$$
z_2 = W_2 x + b_2
$$

$$
a_2 = f(z_2)
$$

$$
...
$$

$$
z_L = W_L x + b_L
$$

$$
a_L = f(z_L)
$$

那么有：
$$
Var(z_L) = Var(x)(\prod_{l=1}^{L} \frac{1}{2} \cdot n_l^{in} \cdot Var(W_l))
$$

$$
Var(\frac{\partial Cost}{\partial x}) = Var(\frac{\partial Cost}{\partial z_l})(\prod_{l=1}^{L} \frac{1}{2} \cdot n_l^{out} \cdot Var(W_l^T))
$$

需要注意的是，我们有：
$$
n_i^{out} = n_{i+1}^{in}
$$
如果，我们只让前向传播的方差不变，也即取$Var(W_l) = \frac{2}{n_{l}^{in}}$，那么反向传播的方差为：
$$
Var(\frac{\partial Cost}{\partial x}) = Var(\frac{\partial Cost}{\partial z_L})(\prod_{l=1}^{L} \frac{1}{2} \cdot n_l^{out} \cdot \frac{2}{n_l^{in}})
$$

$$
Var(\frac{\partial Cost}{\partial x})
=
Var(\frac{\partial Cost}{\partial z_L}) \cdot
\frac{n_1^{out}}{n_1^{in}} \cdot \frac{n_2^{out}}{n_2^{in}} ... \cdot \frac{n_L^{out}}{n_L^{in}}
$$

$$
Var(\frac{\partial Cost}{\partial x})
=
Var(\frac{\partial Cost}{\partial z_L}) \cdot
\frac{n_L^{out}}{n_1^{in}}
$$

这是可以接受的。
同理，如果只让反向传播的方差不变，那么我们可以得到：
$$
Var(z_L) =  \frac{n_1^{in}}{n_L^{out}}
$$


### 实验

下面使用两种方法进行初始化，在全连接网络上进行测试，分别测试了激活函数为`Sigmoid`、`Tanh`和`ReLU`下的效果。

全连接神经网络的参数为`layers_dims = [1000,800,500,300,200,100,90,80,40,20,10]`，除去输入层，神经网络为`10`层，将`10000`个数据输入到该神经网络中，并记录每层输出分布的直方图，下面将这`10`层输出值的直方图画了出来。

使用了两种权重初始化方法

1. Xavier初始化方法，$N(\mu =0, \sigma^2=\frac{2}{n_{in} + n_{out}})$
2. Kaiming初始化方法，$N(\mu =0, \sigma^2=\frac{2}{n_{in}})$

##### 激活函数为Sigmoid

Xavier初始化方法得到的结果：

![image-20190706163638708](../../../assets/权重初始化(3).assert/image-20190706163638708.png)

Kaiming初始化方法得到的结果：

![image-20190706163749415](../../../assets/权重初始化(3).assert/image-20190706163749415.png)

虽然从图可以看出，对于激活函数`sigmoid`而言，kaiming初始化方法比`xavier`初始化方法要好那么一点，但是能够看出趋势，随着网络加深，kaimming初始化方法并不适用于`sigmoid`，毕竟kaiming初始化方法是专为`ReLU`设计的，推导时，也用到了`ReLU`的性质进行推导。

##### 激活函数为Tanh



Xavier初始化方法得到的结果：

![image-20190706164039365](../../../assets/权重初始化(3).assert/image-20190706164039365.png)

Kaiming初始化方法得到的结果：

![image-20190706164058463](../../../assets/权重初始化(3).assert/image-20190706164058463.png)

从图可以看出，对于激活函数`Tanh`而言，两种方法表现都很好，意外的是似乎kaiming初始化方法表现的更好一些。



##### 激活函数为ReLU

Xavier初始化方法得到的结果：

![image-20190706164158566](../../../assets/权重初始化(3).assert/image-20190706164158566.png)

Kaiming初始化方法得到的结果：

![image-20190706164140066](../../../assets/权重初始化(3).assert/image-20190706164140066.png)

由于kaiming初始化是专门为`ReLU`设计的权重初始化方法，所以在激活函数为`ReLU`的深层网络上取得了很好的效果。





### 结论

如果神经网络的激活函数为`ReLU`，那么一定要用kaiming初始化方法，效果显著。如果激活函数为`Tanh`，那么按照理论，应该使用xavier初始化方法，或者$\frac{1}{\sqrt{n_{in}}}$初始化方法，毕竟`Tanh`符合该假设，不过kaiming初始化方法也可以试一试，因为从实验中可以看出kaiming初始化方法在`Tanh`上表现也挺好的。至于激活函数为`Sigmoid`，玄学，毕竟`Sigmoid`都不符合这些初始化方法推导时的假设，等要用了再说。

## 参考

[深度学习之参数初始化（二）——Kaiming初始化 - CodeTutor - CSDN博客](https://blog.csdn.net/VictoriaW/article/details/73166752)
[深度学习中神经网络的几种权重初始化方法 - 天泽28的专栏 - CSDN博客](https://blog.csdn.net/u012328159/article/details/80025785)
[How to initialize deep neural networks? Xavier and Kaiming initialization • Pierre Ouannes](https://pouannes.github.io/blog/initialization/#mjx-eqn-eqfwd_K)





## Appendix

```python
import numpy as np
import matplotlib.pyplot as plt

def initialize_parameters_zeros(layers_dims):
    parameters = {}
    L = len(layers_dims)
    for l in range(1, L):
        parameters['W' + str(l)] = np.zeros((layers_dims[l], layers_dims[l - 1]))
        parameters['b' + str(l)] = np.zeros((layers_dims[l], 1))
    return parameters


def initialize_parameters_random(layers_dims):
    parameters = {}
    L = len(layers_dims)
    for l in range(1, L):
        parameters['W' + str(l)] = np.random.randn(layers_dims[l], layers_dims[l - 1]) * 0.01
        parameters['b' + str(l)] = np.zeros((layers_dims[l], 1))
    return parameters

def initialize_parameters_sqrt(layers_dims):
    parameters = {}
    L = len(layers_dims)
    for l in range(1, L):
        mu = 0
        sigma = np.sqrt(1.0 / layers_dims[l - 1])
        parameters['W' + str(l)] = np.random.normal(loc=mu, scale=sigma, size=(layers_dims[l], layers_dims[l - 1]))
        parameters['b' + str(l)] = np.zeros((layers_dims[l], 1))
    return parameters

def initialize_parameters_xavier(layers_dims):
    parameters = {}
    L = len(layers_dims)
    for l in range(1, L):
        mu = 0
        sigma = np.sqrt(2.0 / (layers_dims[l - 1] + layers_dims[l]))
        parameters['W' + str(l)] = np.random.normal(loc=mu, scale=sigma, size=(layers_dims[l], layers_dims[l - 1]))
        parameters['b' + str(l)] = np.zeros((layers_dims[l], 1))
    return parameters

def initialize_parameters_kaiming(layers_dims):
    parameters = {}
    L = len(layers_dims)
    for l in range(1, L):
        mu = 0
        sigma = np.sqrt(2.0 / layers_dims[l - 1])
        parameters['W' + str(l)] = np.random.normal(loc=mu, scale=sigma, size=(layers_dims[l], layers_dims[l - 1]))
        parameters['b' + str(l)] = np.zeros((layers_dims[l], 1))
    return parameters


def relu(Z):
    A = np.maximum(0, Z)
    return A
def sigmoid(Z):
    A = 1.0 / (1 + np.exp(-Z))
    return A
def tanh(Z):
    A = np.tanh(Z)
    return A

def forward_propagation(initialization='kaiming'):
    data = np.random.randn(1000, 10000)
    layers_dims = [1000, 800, 500, 300, 200, 100, 90, 80, 40, 20, 10]
    num_layers = len(layers_dims)
    if initialization == 'zeros':
        parameters = initialize_parameters_zeros(layers_dims)
    elif initialization == 'random':
        parameters = initialize_parameters_random(layers_dims)
    elif initialization == 'sqrt':
        parameters = initialize_parameters_sqrt(layers_dims)
    elif initialization == 'xavier':
        parameters = initialize_parameters_xavier(layers_dims)
    elif initialization == 'kaiming':
        parameters = initialize_parameters_kaiming(layers_dims)

    A = data
    for l in range(1, num_layers):
        A_pre = A
        W = parameters['W' + str(l)]
        b = parameters['b' + str(l)]
        z = np.dot(W, A_pre) + b # z = Wx + b

        # 不同的激活函数适用不同的初始化方法
        # A = sigmoid(z)
        # A = tanh(z)
        A = relu(z)

        print(A)
        plt.subplot(2, 5, l)
        plt.hist(A.flatten(), facecolor='g')
        # plt.xlim([-1, 1])
        plt.xlim([-5, 5])
        plt.yticks([])
    plt.show()


if __name__ == '__main__':
    '''
    实验不同初始化方法在不同激活函数下的表现
    '''
    # forward_propagation('zeros')
    # forward_propagation('random')
    # forward_propagation('sqrt')
    # forward_propagation('xavier')
    # forward_propagation('kaiming')


```

