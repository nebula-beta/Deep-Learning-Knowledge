[TOC]



# L1/L2正则化



L1/L2就是权重衰减，那么为什么减少权重有利于防止过拟合呢？

权重小的模型比较简单。



![image-20190721153712873](../../../assets/L1 L2正则化.assert/image-20190721153712873.png)





## L1相比于L2为什么能够获得稀疏解

$$
h(w) = f(w) + \lambda |w|
$$

* $h(w)$是目标函数

* $f(w)$是没加正则项的损失函数

* $\lambda |w|$是L1正则项目



既然L1正则化能够获得稀疏解（大部分的权重倾向于0），那么就求目标函数在0点处的导数，看看发生什么。

由于L1正则化在0点处不可导，所以我们求目标函数在0点处的左右导数：
$$
h_{left}'(0) = f'(0) - \lambda
$$

$$
h_{right}'(0) = f'(0) + \lambda
$$

若目标函数在0点处取得极大值，那么就有其在0点的左右导数异号：
$$
h_{left}'(0) \cdot h_{right}'(0) = (f'(0) - \lambda) \cdot (f'(0) + \lambda) < 0
$$
由于$\lambda >0$，所以当$f'(0) < \lambda$或者$f'(0) > -\lambda$时目标函数的左右导数异号。也即
$$
\lambda > |f'(0)|
$$
时导致0点变为极小值点。



> L1正则化，当权重在0点处的变量的导数满足一定的条件时，其0点会变成极小值点。



对于高维优化问题，L1获得稀疏解就是由于$\lambda$相对于某个权重其在原点处导数的绝对值大，从而导致0点变为该权重的极小值点。





而对于L2正则，
$$
h(w) = f(w) + \frac{\lambda}{2} w^2
$$
其在0点处的导数为：
$$
h'(0) = f'(0)
$$

>  并不会像L1正则那样，影响其在0点处的导数。 也就若原先0点不是极小值点，加了L2正则后并不会变成极小值点。










## 参考

[神经网络正则化(1)：L1/L2正则化 - 习翔宇的文章 - 知乎](https://zhuanlan.zhihu.com/p/35893078)

[L1正则化与L2正则化](https://zhuanlan.zhihu.com/p/35356992)

[机器学习中使用正则化来防止过拟合是什么原理？ - Chan.Keh的回答 - 知乎](https://www.zhihu.com/question/20700829/answer/627874879)

[l1 相比于 l2 为什么容易获得稀疏解？](https://www.zhihu.com/question/37096933)