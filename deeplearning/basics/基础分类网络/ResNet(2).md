[TOC]

# ResNet(2)

[Identity Mappings in Deep Residual Networks](http://xxx.itp.ac.cn/abs/1603.05027)



ResNet论文的后续，作者在论文__Identity Mappings in Deep Residual Networks__中分析了残差块背后的反向传播公式。当使用恒等映射作为skip connection时，前向传播和反向传播的信号可以直接从一个块流向另外一个块。






$$
y_l = h(x_l) + F(x_l, W_l)
$$

$$
x_{l+1} = f(x_l)
$$

## 数学推导

[ResNet反向传播公式推导](https://blog.csdn.net/legend_hua/article/details/81741992)



## 恒等映射的重要性

这篇论文使用了不同的映射函数作为skip connection并且验证其效果，最后表面恒等映射的重要性。作者一共比较了6中不同的映射函数。

![image-20190710153234099](../../../assets/ResNet(2).assert/image-20190710153234099.png)



作者在Cifar-10上使用了ResNet-110进行实验。具体结果详见论文，恒等映射取得了最好的结果，test error 为__6.61%__。并且有些映射函数会造成大于20%的test error，这告诉我们映射函数不能乱取。





## 解决激活函数不为恒等映射的问题

论文中使用数学推导证明了skip connection的优越性，但是该证明存在两个假设，一个是假设映射函数是恒等映射，另外一个假设则是假设residual block后的激活函数也是恒等映射。但是，事实上在原来的论文中，residual block后的激活函数是ReLU，并不是恒等映射。


$$
y_l = h(x_l) + F(x_l, W_l)
$$

$$
x_{l+1} = f(x_l)
$$

本节就是说明论文中如何解决这个问题。



![image-20190710154213799](../../../assets/ResNet(2).assert/image-20190710154213799.png)

如图所示，原始的ResNet的残差块在addition之后还有一个ReLU函数作为激活函数。



> 下面是我个人对addition之后存在激活函数存在的问题的理解。
>
> 在我看来，ResNet的insight是深层的网络不应该比渐层的网络差，因为我们可以在浅层的网络之后加上恒等映射而得到一个深层网络。但是这样子虽然深层网络并不比浅层网络差，但是这样子的结构并没有什么意义，所以我们需要在恒等映射的基础上加上一些提取特征的旁路连接模块，然后将这些特征和恒等映射的结果加起来作为下一层的输入。那么我们期望利用标签的监督信息，通过反向传播使得旁路提取特征的模块能够提取到有用的特征从而提高网络的精度，如果无法无法学习有用的特征，那么就为0好了，那么通过主路连接的恒等映射并不会使得深层网咯的性能低于浅层网络。那么我的理解是addition之后是不存在任何激活函数的，这样才是真正的恒等映射，那么恒等映射的结果会被不断激活（例如Sigmoid），那么通向深层网络的信息就不是恒等的了，这和我们的insight不符合，那么激活函数是ReLU，那么是不行，因为负半轴会被抑制。



如果我的分析有道理的话，那么上面图中的5个残差块中，(b)的结果应该劣于(a)，因为addition之后使用ReLU好歹正半轴还是恒等映射，而addition之后使用BN+ReLU的组合，则和完全和恒等映射没有关系。并且(c)、(d)、(e)的addition之后都没有东西，符合之前的分析，所以应该效果不会差。



实验的结果如下：

![image-20190710160224147](../../../assets/ResNet(2).assert/image-20190710160224147.png)

(b)确实是最差的。

(c)为什么没有(a)好呢？论文里说一个残差函数的输出应该是$(-\infty, +\infty)$，如果使用(c)中的结构作为残差函数，那么会导致$F$的输出结果为非负，这会影响表达能力。因为我们希望学到的残差自然是有正有负的，如果全正的，显然就会影响表达能力，也就说由于$F(x)$的非负，从而导致$F(x) + x$就不能生成我们想要的结果。

(d)比之前的好，但是由于ReLU层不和BN层连在一起使用，所以addition之后的结果没有被标准化，这而这个非标准化的信号又被用作下一个权重层的输入。

(e)最好的结果，这是由于在这个版本中，权重层的输入总是先被标准化的。





还有一个问题是，Fig.4 (b)和Fig.4 (c)可以认为是在Fig.4 (a)的基础上改来的，因为变化不大，但是Fig.4 (d)和Fig.4 (e)是怎么想到的呢？其实也可以认为是从原始的Fig.4 (a)修改得到的，因为网络其实有很多block，把这些block画下来，然后选择合适的地方切分一个满足我们要求的block，就得到了下图中的Fig.5 (c)

![image-20190710163941915](../../../assets/ResNet(2).assert/image-20190710163941915.png)





## 参考

[Identity Mappings in Deep Residual Networks（译）](https://blog.csdn.net/wspba/article/details/60750007)