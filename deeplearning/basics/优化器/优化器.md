[TOC]

# 优化器

按照是否自适应学习率，优化器可以分为两类

* 学习率不受梯度的影响： SGD， SGD with Momentum, SGD with Momentum and NAG
* 学习率受梯度影响：Adagrad，Adadelta，RMSprop，Adam

![optim](../../../assets/优化器.assert/optim.gif)



## SGD系列

### SGD

$$
\theta_{t+1} = \theta_{t} - \eta \cdot g_t
$$



SGD的缺点是每次更新的方向可能不会按照正确的方向进行，因此可能带有优化的波动（震荡）。



不过从另外一个方面来看，其波动带来的好处就是能够跳出　当前的局部极小值，从而收敛到一个更好的局部极值点。（batch_size越小，其波动就越大。）

### SGDM

$$
m_t = \gamma m_{t-1} + \eta g_t
$$

$$
\theta_{t+1} = \theta_{t} - m_t
$$

动量物理上的解释就是在碰到谷底时能够有机会反弹，从而跳出局部极小值。

并且加入动量之后还能够减少震荡。

* 当梯度方向和动量方向相同时，动量因子能够加速更新。
* 当梯度方向和动量方向不同时，动量因子能够降低梯度的更新速度，从而避免梯度方向变化太快而引起的震荡。



震荡是指batch_size的梯度方向不准，从而引起的梯度方向变化吗?



### SGDM Nesterov accelerated gradient

$$
m_t = \gamma m_{t-1} + \eta \nabla J(\theta_{t} - \gamma m_{t-1})
$$

$$
\theta_{t+1} = \theta_{t} - m_t
$$

NAG则是先用历史动量走一步，计算梯度。然后将历史动量和梯度相加，从而得到更新方向。



提前用动量走一步的有点在于，若走完之后，梯度是往回走的，那么动量和梯度相加，就会抑制其往那个方向走，从而提前预知不往那个方向走。反之，而加速收敛。



[比Momentum更快：揭开Nesterov Accelerated Gradient的真面目](https://zhuanlan.zhihu.com/p/22810533)　给出了NAG的公式推导，提前走一步取梯度和动量相加等价于利用了当前点的二阶导信息，从而取得比SGD-M更快的收敛速度。



![1565945050324](../../../assets/优化器.asset/1565945050324.png)









## 自适应学习率系列

SGD、SGD-M 和 NAG 均是以相同的学习率去更新 $\theta$的各个分量。而深度学习模型中往往涉及大量的参数，不同参数的更新频率往往有所区别。



### Adagrad

[Adaptive Subgradient Methods for Online Learning and Stochastic Optimization](https://link.zhihu.com/?target=http%3A//www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)
$$
G_t = diag(\sum_{i=1}^{t}g_{i,1}^2, \sum_{i=1}^{t}g_{i,2}^2, \cdots, \sum_{i=1}^{t}g_{i,1}^d )
$$
$G_t  \in R^{d\times d}$是二阶动量对角矩阵，其元素$G_{t, ii}$为参数的第$i$为从初始时刻到$t$时刻的梯度平方和。其参数的更新公式为：
$$
\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{G_t + \epsilon}} \cdot g_t
$$
$\epsilon$是平滑系数，防止除零。此时学习率可以看错是$\frac{\eta}{\sqrt{G_t + \epsilon}}$，可以看出，随着参数的更新，$G_{t, ii}$逐渐增大，$\theta_{t,i}$的学习率逐渐较少。

但是Adagrad有个缺陷就是$G_t$是递增的，这会使得使得学习率逐渐递减至 0，导致参数不再更新，从而可能导致训练过程提前结束。



**起到的效果是在参数空间更为平缓的方向，会取得更大的进步（因为平缓，所以历史梯度平方和较小，对应学习下降的幅度较小），并且能够使得陡峭的方向变得平缓，从而加快训练速度。**



Adagrad适合输入是稀疏的情况，输入是稀疏的是什么意思。



### Adadelta

为了解决Adagrad中$G_t$是递增的，使得学习率逐渐递减至 0，导致参数不再更新的问题，Adadelta不再对过去梯度的平方进行累加，而是使用指数滑动平均的方法计算$G_t$。为了改进这一缺点，可以考虑在计算二阶动量时不累积全部历史梯度，而只关注最近某一时间窗口内的下降梯度。如果某个参数的二阶动量在该窗口内的平均值大，那么其学习率就会变得小一点；反之，则变得大一点。
$$
G_t = \gamma \cdot  G_{t-1} + (1 - \gamma)\cdot diag(g_t^2)
$$


其二阶动量采用指数移动平均公式计算，这样即可避免二阶动量持续累积的问题。和 SGD-M 中的参数类似，$\gamma$通常取$0.9$左右。

### RMSprop

### Adam

[Adam: A Method for Stochastic Optimization](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1412.6980)

Adadelta中RMSprop中使用的还是$\frac{梯度}{二阶动量}$，Adam所做的就是$\frac{梯度+一阶动量}{二阶动量}$
$$
m_t = \eta \cdot [\beta_1 \cdot m_{t-1} + (1 - \beta_1) \cdot g_t ]
$$

$$
G_t = \beta_2 \cdot G_{t-1} + (1 - \beta_2) \cdot diag(g_t^2)
$$

注意到，在迭代初始阶段，$m_t$和$G_t$有一个向初值的偏移（过多的偏向了 0）。因此，可以对一阶和二阶动量做偏置校正 (bias correction)，
$$
\tilde{m}_t = \frac{m_t}{1 - \beta_1^t}
$$

$$
\tilde{G}_t = \frac{G_t}{1 - \beta_2^t}
$$

最后的更新公式为：
$$
\theta_{t+1} = \theta_t - \frac{1}{\sqrt{\tilde{G}_t}+\epsilon} \cdot \tilde{m}_t
$$









指数加权平均的公式为：
$$
v_{t} = \beta \cdot v_{t-1} + (1-\beta) \cdot g_t
$$
初始时是有$v_0=0$，因此刚开始计算时，会有$v_1 = (1-\beta) \cdot g_t$，这就会造成指数加权得到的实际值比真实的平均值要低很多。所以，可以对指数加权平均进行修正，从而得到**带偏差修正的指数加权平均**。
$$
v_{t} = \beta \cdot v_{t=1} + (1-\beta) \cdot g_t
$$

$$
v_t = \frac{v_t}{1-\beta^t}
$$







## 参考

[Deep Learning 最优化方法之AdaGrad](https://zhuanlan.zhihu.com/p/29920135)

[从 SGD 到 Adam —— 深度学习优化算法概览(一)](https://zhuanlan.zhihu.com/p/32626442)

[优化方法总结](https://www.jianshu.com/p/3455dd9487cc)

[比Momentum更快：揭开Nesterov Accelerated Gradient的真面目](https://zhuanlan.zhihu.com/p/22810533)

