# VGGNet

__paper__ : [link](http://xxx.itp.ac.cn/abs/1409.1556)






## 通过堆叠3x3的小卷积核来近似替换


VGG-16有16个卷积层或全连接层，包括五组卷积层和3个全连接层，即：16=2+2+3+3+3+3。这个结构应该牢记于心。

VGG-16则是19=2+2+4+4+4+3


VGGNet的卷积层有一个显著的特点：特征图的空间分辨率单调递减，特征图的通道数单调递增。

这样做是合理的。对于卷积神经网络而言，输入图像的维度是HxWx3（彩色图）或者是HxWx1（灰度图），而最后的全连接层的输出是一个1x1xC的向量，C等于分类的类别数（例如ImageNet中的1000类）。如何从一个HxWx3或HxWx1的图像转换到1x1xC的向量呢？上文所说的VGGNet的特点就是答案：特征图的空间分辨率单调递减，特征图的通道数单调递增，使得输入图像在维度上流畅地转换到分类向量。用于相同ImageNet图像分类任务的AlexNet的通道数无此规律，VGGNet后续的GoogLeNet和Resnet均遵循此维度变化的规律。





## 结论

1. LRN层无性能增益（A-LRN）VGG作者通过网络A-LRN发现，AlexNet曾经用到的LRN层（local response normalization，局部响应归一化）并没有带来性能的提升，因此在其它组的网络中均没再出现LRN层。
2. 随着深度增加，分类性能逐渐提高（A、B、C、D、E）
3. 多个小卷积核比单个大卷积核性能好（B）


## 参考

[精读深度学习论文(1) VGG - 知乎](https://zhuanlan.zhihu.com/p/33275704)