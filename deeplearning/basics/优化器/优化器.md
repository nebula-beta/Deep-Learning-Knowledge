[TOC]

# 优化器

按照是否自适应学习率，优化器可以分为两类

* 学习率不受梯度的影响： SGD， SGD with Momentum, SGD with Momentum and NAG
* 学习率受梯度影响：Adagrad，Adadelta，RMSprop，Adam

![optim](../../../assets/优化器.assert/optim.gif)



## SGD系列

### SGD

$$
\theta_{t+1} = \theta_{t} - \eta \cdot g_t
$$



### SGDM

$$
m_t = \gamma m_{t-1} + \eta g_t
$$

$$
\theta_{t+1} = \theta_{t} - m_t
$$



### SGDM Nesterov accelerated gradient

$$
m_t = \gamma m_{t-1} + \eta \nabla J(\theta_{t} - \gamma m_{t-1})
$$

$$
\theta_{t+1} = \theta_{t} - m_t
$$





## 自适应学习率系列

SGD、SGD-M 和 NAG 均是以相同的学习率去更新 $\theta$的各个分量。而深度学习模型中往往涉及大量的参数，不同参数的更新频率往往有所区别。



### Adagrad

[Adaptive Subgradient Methods for Online Learning and Stochastic Optimization](https://link.zhihu.com/?target=http%3A//www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)
$$
G_t = diag(\sum_{i=1}^{t}g_{i,1}^2, \sum_{i=1}^{t}g_{i,2}^2, \cdots, \sum_{i=1}^{t}g_{i,1}^d )
$$
$G_t  \in R^{d\times d}$是二阶动量对角矩阵，其元素$G_{t, ii}$为参数的第$i$为从初始时刻到$t$时刻的梯度平方和。其参数的更新公式为：
$$
\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{G_t + \epsilon}} \cdot g_t
$$
$\epsilon$是平滑系数，防止除零。此时学习率可以看错是$\frac{\eta}{\sqrt{G_t + \epsilon}}$，可以看出，随着参数的更新，$G_{t, ii}$逐渐增大，$\theta_{t,i}$的学习率逐渐较少。

但是Adagrad有个缺陷就是$G_t$是递增的，这会使得使得学习率逐渐递减至 0，导致参数不再更新，从而可能导致训练过程提前结束。



**起到的效果是在参数空间更为平缓的方向，会取得更大的进步（因为平缓，所以历史梯度平方和较小，对应学习下降的幅度较小），并且能够使得陡峭的方向变得平缓，从而加快训练速度。**



Adagrad适合输入是稀疏的情况，输入是稀疏的是什么意思。



### Adadelta

为了解决Adagrad中$G_t$是递增的，使得学习率逐渐递减至 0，导致参数不再更新的问题，Adadelta不再对过去梯度的平方进行累加，而是使用指数滑动平均的方法计算$G_t$。为了改进这一缺点，可以考虑在计算二阶动量时不累积全部历史梯度，而只关注最近某一时间窗口内的下降梯度。
$$
G_t = \gamma \cdot  G_{t-1} + (1 - \gamma)\cdot diag(g_t^2)
$$


其二阶动量采用指数移动平均公式计算，这样即可避免二阶动量持续累积的问题。和 SGD-M 中的参数类似，$\gamma$通常取$0.9$左右。

### RMSprop

### Adam

[Adam: A Method for Stochastic Optimization](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1412.6980)

Adadelta中RMSprop中使用的还是$\frac{梯度}{二阶动量}$，Adam所做的就是$\frac{梯度+一阶动量}{二阶动量}$
$$
m_t = \eta \cdot [\beta_1 \cdot m_{t-1} + (1 - \beta_1) \cdot g_t ]
$$

$$
G_t = \beta_2 \cdot G_{t-1} + (1 - \beta_2) \cdot diag(g_t^2)
$$

注意到，在迭代初始阶段，$m_t$和$G_t$有一个向初值的偏移（过多的偏向了 0）。因此，可以对一阶和二阶动量做偏置校正 (bias correction)，
$$
\tilde{m}_t = \frac{m_t}{1 - \beta_1^t}
$$

$$
\tilde{G}_t = \frac{G_t}{1 - \beta_2^t}
$$

最后的更新公式为：
$$
\theta_{t+1} = \theta_t - \frac{1}{\sqrt{\tilde{G}_t}+\epsilon} \cdot \tilde{m}_t
$$






## 参考

[Deep Learning 最优化方法之AdaGrad](https://zhuanlan.zhihu.com/p/29920135)

[从 SGD 到 Adam —— 深度学习优化算法概览(一)](https://zhuanlan.zhihu.com/p/32626442)

[优化方法总结](https://www.jianshu.com/p/3455dd9487cc)



