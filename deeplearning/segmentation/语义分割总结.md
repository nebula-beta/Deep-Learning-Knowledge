

[TOC]

## 语义分割总结

### 语义分割中的挑战

语义分割存在的挑战：

* 多尺度物体
* 物体定位的准确性
* 物体边界的准确性



图像分类任务和图像语义分割任务是存在一定差异的，所以直接把分类的技巧用到语义分割这里，并不会得到很好的性能。分类任务的特性在于不断提取特征，组合特征，从而得到越来越抽象的特征，而最后用于分类的全连接层只在乎图像中存不在该特征，并不在乎特征位于图像的哪个位置。



而对于图像语义分割任务，可以看做是逐像素的分类任务，对于每个像素的分类，这部分可以参考图像分类来进行设计。但是如何直接参考图像分类设计的话，就会引入另外一个问题，由于图像分类网络会对特征图进行降采样，一般来说是降采样32倍。比如224 x 224的原始图像经过32倍降采样后得到7 x 7大小的特征图。



降采样操作可以扩大感受野因而能够很好地整合上下文信息（context中文称为语境或者上下文，通俗的理解就是综合了更多的信息来进行决策），对high-level的任务（比如分类），这是很有效的。但同时，由于降采样使得分辨率降低，因此削弱了位置信息，而语义分割中需要score map和原图对齐，因此需要丰富的位置信息。



下面来分析下，降采样到底是如何丢失位置信息的。以224 x 224的原数图像降采样32倍得到7 x 7大小的特征图（score map）为例子。然后将标签也降采样32倍，在7 x 7的特征图上（score map）构建损失，进行训练，那么最后得到的预测结果也是7 x 7的。那么为了得到原图大小的预测结果，就需要上采样32倍，那么score map中的1个点就对应原图像32 x 32的区域（这个说法不是很准确，因为上采样的时候用到了插值算法），这就造成物体边界的不准确和物体定位的不准确（如何理解定位）。





### FCN

随着一次次的池化，虽然感受野不断增大，语义信息不断增强。但是池化造成了像素位置信息的丢失：直观举例，1/32大小的Heatmap上采样到原图之后，在Heatmap上如果偏移一个像素，在原图就偏移32个像素，这是不能容忍的。



为了解决这个问题，FCN所用的策略就是将1/8，1/16，1/32的三个层的输出融合起来。先把1/32的输出上采样到1/16，然后和pool4的输出做element-wise addition，结果再上采样到1/8，然后和pool3的输出做element-wise addition。从而得到1/8的结果，然后上采样8倍，最后和标签求Loss。



高层的特征比较抽象，但是丢失了大量的位置信息，低层的特征没那么抽象，但是保留了一定的位置信息。FCN中所提出的这个方式来解决网络不断池化所带来的位置信息的丢失，贯穿了整个语义分割领域。

*Benchmarks (VOC2012)*:

| Score | Comment                               | Source                                                       |
| ----- | ------------------------------------- | :----------------------------------------------------------- |
| 62.2  | -                                     | [leaderboard](http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?cls=mean&challengeid=11&compid=6&submid=6103#KEY_FCN-8s) |
| 67.2  | More momentum. Not described in paper | [leaderboard](http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?cls=mean&challengeid=11&compid=6&submid=6103#KEY_FCN-8s-heavy) |



### UNet

其遵从的原理和FCN一致，相比于FCN，其有两个改进：

* 更加丰富的信息融合，更多的前后层之间的信息融合。
* 区别于FCN的deconv + element-wise addition，UNet使用的是concat + conv，可以让卷积核自己在channel上作出选择。



### SegNet

SegNet虽然也遵从了FCN的encoder-decoder的结构，但是并没有和FCN一样通过融合不同层次的信息来解决上采样时的定位问题。SegNet通过记录每次池化操作时的最大值所对应的下标，在上采样时根据坐标将最大值放回到原来的位置的方式来处理上采样时的定位问题。SegNet和FCN的区别在于：

* 上采样时根据最大值的下标将最大值放回到原来的位置；
* 放回之后，由于池化是2x2大小的，所以导致4个元素中一个有值，其它3个为0。SegNet在反池化之后接conv来处理这个问题。



*Benchmarks (VOC2012)*:

| Score | Comment                      | Source                |
| ----- | ---------------------------- | :-------------------- |
| 71.3  | frontend                     | reported in the paper |
| 73.5  | frontend + context           | reported in the paper |
| 74.7  | frontend + context + CRF     | reported in the paper |
| 75.3  | frontend + context + CRF-RNN | reported in the paper |



### DeepLab

DeepLab的前3个系列虽然没有使用FCN所提出来的跨层连接来解决上采样的定位问题，但是V3+确实使用了该方案，这也说明FCN确实厉害。

#### V1

语义分割有两个难以权衡的地方，既要进行分类，那么就需要进行不断卷积+池化提出高层抽象特征；但是又要进行逐像素分类，这就要求不能降采样太多了，不然就会损失很多的位置信息。

一个自然的想法就是我不进行降采样，而是通过不断扩大卷积核的方式来扩大感受野，从而提取更大区域和更加抽象的特征。嗯，不进行降采样，又要扩大卷积核大小，那么实际是行不通的，计算量是无法接受的。

但是，我感觉DeepLab就是遵从了这个思想：

* 只不过在[不进行降采样， 降采样32倍]这个区间中做了一个权衡，降采样8倍。
* 而在扩大感受野上，其选择不是增大卷积核，而是提出了带孔卷积核。

*Benchmarks (VOC2012)*:

| Score | Comment                         | Source                                                       |
| ----- | ------------------------------- | :----------------------------------------------------------- |
| 71.6  | VGG + atrous Convolutions + CRF | [leaderboard](http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?cls=mean&challengeid=11&compid=6&submid=4646#KEY_DeepLab-MSc-CRF-LargeFOV) |

#### V2

DeepLab-v1由于只降采样了8倍，没有损失很多的位置信息，所以其没有像FCN一样融合不同层次的信息。但是融合不同层次的信息除了处理池化丢失位置信息的问题之外，还有一个功能就是融合多尺度的特征，从而处理图像中存在不同尺度物体的分割问题。

所以在DeepLab-v2中，为了提取多尺度的特征，其提出了一个ASPP（Atrous Spatial Pyramid Pooling）结构。ASPP层就是为了融合不同尺度的语义信息：选择不同扩张率的带孔卷积去处理Feature Map，由于感受野不同，得到的信息的尺度也就不同。ASPP层把这些不同层级的feature map concat到一起，进行信息融合。

*Benchmarks (VOC2012)*:

| Score | Comment                                       | Source                                                       |
| ----- | --------------------------------------------- | :----------------------------------------------------------- |
| 79.7  | ResNet-101 + atrous Convolutions + ASPP + CRF | [leaderboard](http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?cls=mean&challengeid=11&compid=6&submid=6103#KEY_DeepLabv2-CRF) |

#### V3

V3相比于V2的改动在于

* 改进了ASPP模块，解决了atrous conv扩展率多大时出现退化的问题，并加入了全局平均池化来聚合全局信息；
* 去除了dense-CRF
* 分别探索了降采样8倍和16倍的性能，当然是8倍的好，但是17倍能够降低计算量。



#### V3+

之前的DeepLab-v1和v2是将特征图降采样到8倍，当然了你也可以降采样到4倍。但是降采样的倍数少了，计算就会很密集，因为原来是在降采样16倍或者32倍的特征图上进行卷积操作，现在变成了在降采样8倍的特征图上进行操作，明显是慢了。

DeepLab一开始的卖点就是不降采样32倍，而是降低采样8倍，使用空洞卷积从而获得更好的效果，（而且还达到了8frame/s的效果？DeepLab-v1论文说的，只有就不再提了，可能之后加了金字塔，就变慢了）但是现在加了太多东西了，好像越来越慢了？但是为了能够实用，所以就在DeepLab-v3中尝试了降采样8倍和16倍的效果。 明显降采样8倍的效果比降采样16倍的效果好。DeepLab-v3中只是做了实验，并没有解决这个问题。

本文中将这个问题明确提了出来，降采样倍数少了，计算量就更大了。而降采样倍数多了，效果就不好了。而解决方案就是，不像FCN,UNet那么降采样32倍，而是降采样16倍，降采样倍数多了，那么定位就不准了，所以边界分割就不好了，为了解决这个问题，自然就是加入解码结构，在上采样的过程中融合底层的特征，从而减少大倍数降采样所造成的位置信息的丢失。



### PSPNet

### GCN — Larget  Kernel



 





## 参考

[图像语义分割综述](https://zhuanlan.zhihu.com/p/37801090)

[细节问题思考](https://www.cnblogs.com/ymjyqsx/p/9585803.html)