[TOC]



## 数据如何准备

### 开源数据集

[Brown dataset](http://matthewalunbrown.com/patchdata/patchdata.html)，分为3个不同的场景：

- liberty，40多万个patch
- noteame，40多万个patch
- yosemite，60多万个patch



140多万个patch， 对应6000张图像，每张图像1个G，共6个G左右



每张图像的大小是1024 * 1024，每张图像包含了16 x 16个patch，每个patch的大小是64 * 64大小的灰度图。将图像中的patch按照从左到右，从上到下的顺序编号。info.txt文件中描述了patch之间的匹配关系，如果不同的patch对应同一个3D点，那么其就是匹配的patch。 Info.txt的第i行描述了有两个数字，描述了第i个patch所对应的3D点。



[Oxford dataset](http://www.robots.ox.ac.uk/~vgg/data/data-aff.html)

HPatches dataset

### 自己的数据集



**数据集生成**：三维重建后，能够知道每个3D点所对应的2D点，同一个3D点所对应的2D点都认为是匹配点（当然不可能保证百分百是正确匹配）。根据每个2D点的特征点所对应的角度和尺度，根据尺度，能够知道每个特征点需要多大的图像区域来计算特征描述子，但是神经网络需要一致大小的输入，所以将该区域resize到32x32。并且根据特征点的角度对patch进行旋转。



一个3D点对应的2D点可能多，也就说这些2D点的匹配关系是稳定的，错误匹配的概率低。所以刚开始认为从匹配点多的2D点中取patch。

~~后来经过思考后，在一个3D点对应的2D点少的情况下；有两种情况，匹配难度大，或者是错误匹配。匹配难度大的样本对于训练是有必要的，所以需要从中选取patch。但是为了避免选到错误的匹配，要对选取的对应patch进行一定的校验，（人眼过一遍进行校验）。~~



并且大部分的3D点对应的2D点的数目都是比较少的。



选择匹配点最少为3的patch作为数据集



最终，选取出50万个左右个3D点，250万左右个patch。

patch大小为32 x 32，每张图像16 x 16 = 196个patch，每张图像的大小为1024 x 1024。

最后总共有1w多张图像，每张图像1M，所以共10个G左右的数据集。

灰度图是10个G，那么彩色图呢。

彩色图，每张图像2.5M左右，所以彩色数据集是20多G大小， 也就是灰度图数据集的2点多倍。





## 训练设置

batch size 为 1024或者512。大的batch size对triplet loss是有利的。

优化器使用SGD，学习率随着迭代次数的增加而线性减小。

测试的时候，每秒跑4次迭代，每次迭代1024个batch_size， 也就是每秒跑4096个patch，由于每张图像196个patch，所以每秒20多张图像左右。



hardnet是16w个3D点，生成了5百万个triplet，训练一个epoch20多分钟。



实际场景测试时，因为我们的图像比较大，每个图像5000到1万个特征点，那么每张图像需要1到2s来计算描述子。



学习率的调整方案：

优化器使用的是SGD， 首先确定初始化的学习率， 然后我发现，学习率可以设为10这么大， 并且收敛得很快。 之所以学习率可以设置得比较大， 我分析是因为我用了BN，更主要的是我的网络比较浅，只有几层。



然后接下来的调整使用了两种方案，  一种就是不断迭代，等loss不下降了然后将学习率除以10，另外一种就是poly学习率调整。  后者比前者要好一些，







## 网络结构

L2-Net中说BN不能够使用仿射变换，　所以HardNet中也是不用的，我改了一下用仿射变换，发现好像没什么影响。



为什么用triplet loss比用softmax 好呢呢？



对于分类任务，其用softmax + 交叉熵， 那么分类任务有什么特征点，首先，标签的类别数量是固定的，对于某一个张图像，只需要最大化其标签的概率就行了。

但是对于度量相似性这种任务，softmax + 交叉熵损失就不是很好。标签类别有几十万个，对于分类任务而已，类别越多越不好越好优化， 这是一个理解的角度。



另外一个角度是从平均的角度。 999个小loss + 1个比较大的loss，那么求导之后，得到999个小梯度 + 1个比较大的梯度，平均一下就使的平均梯度变小。 这使得没用使用hard mining的训练比较难以学到东西。 所以改成triplets loss之后，有了hard mining之后，就容易学到东西，从而加速网络的收敛。







## 如何构造loss/如何挑选batch中的数据构造样本对

使用了triplet loss + online batch mining



对于每个batch，其batch中的每个元素是一个匹配对，对于一个batch算出一个距离矩阵，其对角线上的元素是匹配对的距离，非对角线的元素是非匹配对的距离。我们在这个矩阵上构造loss。



一个想法就是希望对角线元素的距离尽可能小，非对角线元素的距离尽可能大。



但是，其实，我们只需要保证匹配对的距离小于非匹配对的距离就行了。

所以，我们只需要优化那些距离比较小的非匹配对就可以了。



更进一步，只去优化那些距离最小的非匹配样本就行了。









Hard Negative Mining能够加快收敛速度。为什么能够加快收敛速度呢？

HardNet用的是hardest in batch策略，比L2Net的softmax策略的收敛的收敛速度要快很多，并且L2Net如果单纯只使用一个softmax，那么其会不收敛。



对于学习特征描述子这个任务，正负样本的比例是及其不均衡的。若数据集中有p对匹配对，那么就有$p^2-p$对非匹配对。p越大，正负样本的比例就越不均衡。



L2Net是对一行和一列取softmax后，最小化正类的概率，从而最小化正类之间的距离。 问题就出在这个最小化概率上。 对于分类任务，最大化softmax之后的概率，由于概率和为1的限制，从而只允许一个大的概率，其它概率都是小的。若是最小化概率，那么并不能保证其它概率都是大的，也即不能保证非匹配对的概率都是大的。也就是说导致其匹配对和非匹配对之间的距离的maigin可能不会很大，这导致其性能并不是很好(不收敛)。







为什么triplet比softmax收敛快呢，

我觉得可能是triplet有hard mining的机制在里面，使得每次迭代时，都能使得网络学到很多的信息，这就使其比softmax收敛快，另外一个就是margin， 使得不仅要分开，还要大于margin，





**margin如何取**

尝试了几个margin的取值，取1.0比较好。





## 如何评价性能

FPR95 ： 在recall为0.95时，错误匹配的数量占据总匹配数量的多少。

用这个评价指标的意义在于， 我们希望能够尽可能特征点匹配的查全率和查准率都尽可能高。这样才能够提高三维重建的重建完整率。



FPR = FP / (FP + TN)



运行时间

mAP : 如果有时间， 那么把这个也加入进去。





## 难点

数据集的准备

网络结构的设计(直接使用L2Net的网络结构， 我主要的贡献是修改了Loss）

Loss的设计（正负样本不均衡， 参考目标检测或者是人脸识别）

学习率的调整

评价指标的选择







[Triplet Network, Triplet Loss及其tensorflow实现](https://zhuanlan.zhihu.com/p/35560666)

[Facenet即triplet network模型训练，loss不收敛的问题？](https://www.zhihu.com/question/38937343)

[Understanding Ranking Loss, Contrastive Loss, Margin Loss, Triplet Loss, Hinge Loss and all those confusing names](https://gombru.github.io/2019/04/03/ranking_loss/)

[Understanding Categorical Cross-Entropy Loss, Binary Cross-Entropy Loss, Softmax Loss, Logistic Loss, Focal Loss and all those confusing names](https://gombru.github.io/2018/05/23/cross_entropy_loss/)



[Siamese Network & Triplet Loss](https://towardsdatascience.com/siamese-network-triplet-loss-b4ca82c1aec8)